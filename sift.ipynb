{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   5%|▌         | 1/20 [00:00<00:02,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 20/20 [00:01<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "descpt_id = 'sift'\n",
    "images_path = '/home/garima/Desktop/Beer_images/distances/database'\n",
    "images = os.listdir(images_path)\n",
    "images = [os.path.join(images_path, item) for item in images]\n",
    "\n",
    "if descpt_id == 'sift':\n",
    "    feature_extractor = cv2.xfeatures2d.SIFT_create()\n",
    "elif descpt_id == 'surf':\n",
    "    feature_extractor = cv2.xfeatures2d.SURF_create(extended=True)   # For getting descriptor of size 128 instead of 64\n",
    "else:\n",
    "    feature_extractor = cv2.ORB_create()\n",
    "print('[INFO] Creating descriptors...')\n",
    "descriptors = dict()\n",
    "for image in tqdm(images, desc='Processing images: '):\n",
    "    image_gray = cv2.imread(image, 0)\n",
    "    image_gray = cv2.resize(image_gray,(400,300))\n",
    "    try:\n",
    "        keypoint, descriptor = feature_extractor.detectAndCompute(image_gray, None)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pdb.set_trace()\n",
    "    image_name = image.split('/')[-1].split('.')[0]\n",
    "    if image_name not in descriptors.keys():\n",
    "        descriptors[image_name] = descriptor\n",
    "    else:\n",
    "        print('[ERROR] Issue with file {}!\\nCheck for duplicate images.'.format(image))\n",
    "with open(descriptors1, 'wb+') as file:\n",
    "    pickle.dump(descriptors, file)\n",
    "# else:\n",
    "#     print('[INFO] Using previously saved descriptors...')\n",
    "#     with open(args.descriptors, 'rb') as file:\n",
    "#         descriptors = pickle.load(file)\n",
    "if len(descriptors.keys()) != len(images):\n",
    "    raise ValueError(\"Number of descriptors do not match with number of images in the dataset folder. Create a new set of descriptors.\")\n",
    "else:\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre Computed Descriptors\n",
      "--Database Descriptors Loaded--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:01<00:11,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] database_image: miller_lite2, query_image: storm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:12<00:08,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] database_image: kingfisher, query_image: kingfisher_strong\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:13<00:07,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] database_image: miller_lite2, query_image: betsy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:14<00:05,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] database_image: bira_blonde, query_image: heineken\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:19<00:01,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] database_image: corona_extra, query_image: kingfisher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 20/20 [00:19<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Correct matches: 15 total queries: 20 Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "l = '..distance_sift.pkl'\n",
    "load = 'Y'\n",
    "GOOD_MATCH_THRESH = 10\n",
    "\n",
    "query_images = '../query/'\n",
    "database_images = '../database/'\n",
    "descriptor_file = l\n",
    "load_descriptor =load\n",
    "\n",
    "database_descriptors = {}\n",
    "name_list = []\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "#feature_extractor = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "if load_descriptor == 'Y' or load_descriptor =='y':\n",
    "    print(\"Loading Pre Computed Descriptors\")\n",
    "    assert os.path.exists(descriptor_file)\n",
    "    with open(descriptor_file, 'rb') as file:\n",
    "        database_descriptors = pickle.load(file)\n",
    "else:\n",
    "    for image_name in tqdm(os.listdir(database_images)):\n",
    "        img = cv2.imread(database_images+image_name,0)\n",
    "        keypoint, descriptor = sift.detectAndCompute(img, None)\n",
    "        img_name,img_extension = os.path.splitext(image_name)\n",
    "        database_descriptors[img_name] = descriptor\n",
    "        name_list.append(img_name)\n",
    "    with open(descriptor_file, 'wb') as file:\n",
    "        pickle.dump(database_descriptors, file)\n",
    "\n",
    "print(\"--Database Descriptors Loaded--\")\n",
    "\n",
    "correct_count = 0\n",
    "\n",
    "\n",
    "for query_name in tqdm(os.listdir(query_images)):\n",
    "    max_count_name = None\n",
    "    max_count = 0\n",
    "    query = cv2.imread(query_images+query_name, 0)\n",
    "    _, q_desc = sift.detectAndCompute(query, None)\n",
    "    for database_name in database_descriptors.keys():\n",
    "        d_desc = database_descriptors[database_name]\n",
    "        FLANN_INDEX_KDTREE = 0\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks = 100)\n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = flann.knnMatch(q_desc, d_desc, k=2)\n",
    "        matches_mask = [[0, 0] for i in range(len(matches))]\n",
    "        distances = list()\n",
    "        counts = list()\n",
    "        for i, (m, n) in enumerate(matches):\n",
    "            if m.distance < 0.7 * n.distance:\n",
    "                matches_mask[i] = [1, 0]\n",
    "                distances.append(m.distance)\n",
    "        counts.append(len(distances))\n",
    "        if len(distances) != 0:\n",
    "            if np.max(counts) > max_count:\n",
    "                max_count = np.max(counts)\n",
    "                max_count_name = database_name.split('/')[-1].split('.')[0]\n",
    "    if query_name.split('/')[-1].split('.')[0] == max_count_name:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        print('[ERROR] database_image: {}, query_image: {}'.format(max_count_name, query_name.split('/')[-1].split('.')[0]))\n",
    "print('[INFO] Correct matches: {} total queries: {} Accuracy: {}'.format(correct_count, len(os.listdir(query_images)), correct_count/len(os.listdir(query_images))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
